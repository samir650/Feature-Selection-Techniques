# Features selection / Feature Importance

Feature selection is a crucial step in the machine learning pipeline that involves choosing a subset of relevant features to build a model. An effective feature selection process can enhance model performance, reduce overfitting, and speed up training

Therefore, I made a notebook containing the special code for the most famous techniques for Features selection, and I explained how each of them works and when to use them.


- The techniques are explained : 

1 - Correlation Analysis

2 - Univariate Feature Selection ( chi-squared test - F-test )

3 -  Feature Importance from Models

4 - Principal Component Analysis (PCA)

5 - Mutual Information 
